{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download all relevant images from a product page on Pas Normal Studios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://datascience:****@pkgs.dev.azure.com/dlimi/datascience/_packaging/datascience/pypi/simple/\n",
      "Requirement already satisfied: selenium in /home/ice/.local/lib/python3.10/site-packages (4.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ice/.local/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in /home/ice/.local/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/lib/python3/dist-packages (from selenium) (1.26.5)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /home/ice/.local/lib/python3.10/site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: trio~=0.17 in /home/ice/.local/lib/python3.10/site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/ice/.local/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /home/ice/.local/lib/python3.10/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/ice/.local/lib/python3.10/site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ice/.local/lib/python3.10/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ice/.local/lib/python3.10/site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /home/ice/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/ice/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/ice/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: outcome in /home/ice/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sortedcontainers in /home/ice/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/ice/.local/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/ice/.local/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/ice/.local/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install selenium beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs\n",
    "# URL of the product page\n",
    "BASE_URL = \"https://pasnormalstudios.com/dk/\"\n",
    "PRODUCT_URL = \"products/\"\n",
    "PRODUCTS = [\n",
    "    \"off-race-logo-t-shirt-grape\",\n",
    "    \"off-race-pants-light-brown\",\n",
    "    \"off-race-bandana-classic-red\",\n",
    "    \"off-race-cotton-twill-pants-limestone\",\n",
    "    \"off-race-cap-dusty-orange\",\n",
    "    \"off-race-cap\",\n",
    "    \"off-race-logo-sweatshirt\",\n",
    "    \"pns-x-diemme-movida92-sand\",\n",
    "    \"off-race-logo-hoodie\",\n",
    "    \"off-race-logo-t-shirt-smoke-green\",\n",
    "    \"off-race-pants-classic-blue\",\n",
    "    \"off-race-pants-black\",\n",
    "    \"off-race-pants-off-white\",\n",
    "    \"off-race-pants-beige\",\n",
    "    \"off-race-pants-deep-green\",\n",
    "]\n",
    "\n",
    "# Image lirary\n",
    "# Extract company name dynamically (e.g., \"pasnormalstudios\")\n",
    "company_name = urlparse(BASE_URL).netloc.split('.')[0]\n",
    "\n",
    "# Construct the directory path\n",
    "save_directory = os.path.join(\"../images\", company_name, PRODUCT_URL)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Selenium with headless Chrome\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(\"/usr/local/bin/chromedriver\")\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_urls_from_carousel(soup, URL):\n",
    "    \"\"\"Extract image URLs from the carousel\"\"\"\n",
    "\n",
    "    # Find the carousel container (adjust the selector based on the actual HTML structure)\n",
    "    carousel = soup.find(\"div\", class_=\"swiper-wrapper\")\n",
    "\n",
    "    image_urls = []\n",
    "\n",
    "    img_tags = carousel.find_all(\"img\")\n",
    "    for img in img_tags:\n",
    "        img_url = img.get(\"src\")\n",
    "\n",
    "        if img_url:\n",
    "            # Construct the full URL\n",
    "            full_url = urljoin(URL, img_url.split(\"?\")[0])\n",
    "\n",
    "            # Download and save the image\n",
    "            try:\n",
    "                image_urls.append(full_url)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {full_url}: {e}\")\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "def extract_image_urls_from_lazy_selector(soup, URL):\n",
    "    \"\"\"Extract image URLs from the lazy-loaded selector\"\"\"\n",
    "\n",
    "    image_urls = []\n",
    "\n",
    "    # Locate the specific div that contains the lazy-loaded images\n",
    "    lazy_selector_div = soup.select_one(\"div.order-1.flex.flex-row-reverse.items-end.justify-end.gap-2.md\\\\:order-2.md\\\\:flex-row\")\n",
    "        \n",
    "    if not lazy_selector_div:\n",
    "        print(\"Lazy selector div not found.\")\n",
    "        return image_urls  # Return an empty list if the div isn't found\n",
    "\n",
    "    # Extract the currently displayed main image\n",
    "    main_image = lazy_selector_div.select_one(\"figure img\")\n",
    "    if main_image and main_image.get(\"src\"):\n",
    "        full_url = urljoin(URL, main_image[\"src\"].split(\"?\")[0])\n",
    "        image_urls.append(full_url)\n",
    "\n",
    "    # Extract all thumbnails in the lazy selector\n",
    "    thumbnails = lazy_selector_div.select(\".cursor-pointer img\")  # Adjust selector if necessary\n",
    "\n",
    "    for img in thumbnails:\n",
    "        img_url = img.get(\"src\")\n",
    "        if img_url:\n",
    "            full_url = urljoin(URL, img_url.split(\"?\")[0])\n",
    "            image_urls.append(full_url)\n",
    "\n",
    "    return list(set(image_urls))  # Remove duplicates\n",
    "\n",
    "\n",
    "def save_image_from_url(url, product_directory, key):\n",
    "    img_name = os.path.basename(url)\n",
    "\n",
    "    # Make directory\n",
    "    image_location_directory = os.path.join(product_directory, key)\n",
    "    os.makedirs(image_location_directory, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(image_location_directory, img_name)\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(1024):   \n",
    "                file.write(chunk)\n",
    "        \n",
    "        print(f\"Downloaded: {img_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Products:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening: https://pasnormalstudios.com/dk/products/off-race-pants-beige\n",
      "Downloaded: 3428d29fd62ba02207bbe21768cad66da0ef5baf-3000x3750.png\n",
      "Downloaded: 4ae4dee0dcf31a92419783d72d78fa0762bfd2e8-3200x4000.jpg\n",
      "Downloaded: 0fea28883bce14020acf602ea151527aea477059-3200x4000.jpg\n",
      "Downloaded: 8b9e1d8037c03fdd6fad1638ef7567cbf3a248d8-3000x3750.png\n",
      "Downloaded: 235d62ab9f1ee46b8196a7bca725e7662cbe479a-3000x3750.png\n",
      "Downloaded: e0d8d9f27f5bcc8f10247eda6657ec6fb3cc409d-2048x2560.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Products:  50%|█████     | 1/2 [00:32<00:32, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: 4d42faf97ea5d99ca3fdec453a8415cca2f0b040-2048x2560.jpg\n",
      "Opening: https://pasnormalstudios.com/dk/products/off-race-pants-deep-green\n",
      "Downloaded: 590b861999ee01c385c9475c7aa772da946e8244-3000x3750.png\n",
      "Downloaded: 001dc77bec1d3631f31a215cedb5df0c1107e967-3200x4000.jpg\n",
      "Downloaded: d298ee74f8552859f5153c1be9a990cd08501ecd-3000x3750.png\n",
      "Downloaded: c0fb70fe20a62f7b6433c0c8230d4761ec54ee2b-3000x3750.png\n",
      "Downloaded: 25b3ccc6367e0c7ad8643d52a3a277c010979b58-2048x2560.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Products: 100%|██████████| 2/2 [00:39<00:00, 19.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: 5ba7f75fd0a9680ebaa669819b3cd3edaa8c21d9-2048x2560.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for product in tqdm(PRODUCTS, desc=\"Processing Products\"):\n",
    "    URL = urljoin(urljoin(BASE_URL, PRODUCT_URL), product)\n",
    "\n",
    "    # Open the webpage\n",
    "    print(f'Opening: {URL}')\n",
    "    driver.get(URL)\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"figure img\"))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {URL}: {e}\")\n",
    "        continue  # Skip to the next URL if loading fails\n",
    "\n",
    "    # Make product directory\n",
    "    product_directory = os.path.join(save_directory, product)\n",
    "    os.makedirs(product_directory, exist_ok=True)\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    image_urls = {\n",
    "        \"carousel\": extract_image_urls_from_carousel(soup, URL),\n",
    "        \"lazy_selector\": extract_image_urls_from_lazy_selector(soup, URL)\n",
    "    }\n",
    "    \n",
    "    for key, urls in image_urls.items():\n",
    "        for url in urls:\n",
    "            save_image_from_url(url, product_directory, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Selenium browser\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
