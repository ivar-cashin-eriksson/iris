{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e129960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0d18b",
   "metadata": {},
   "source": [
    "# Text Embeddings with OpenCLIP\n",
    "\n",
    "This notebook demonstrates how to generate text embeddings using OpenCLIP, similar to how we handle image embeddings in our existing codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "from pymongo import MongoClient\n",
    "import numpy as np\n",
    "from iris.config.embedding_pipeline_config_manager import EmbeddingPipelineConfigManager\n",
    "from iris.config.data_pipeline_config_manager import DataPipelineConfigManager\n",
    "from iris.embedding_pipeline.embedding_database import EmbeddingDatabase\n",
    "from iris.data_pipeline.mongodb_manager import MongoDBManager\n",
    "import iris.utils.data_utils as data_utils\n",
    "\n",
    "\n",
    "\n",
    "# Initialize CLIP model\n",
    "model_name = \"ViT-B-32\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name,\n",
    "    pretrained=\"laion2b_s34b_b79k\",\n",
    "    device=device\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Initialize embedding database\n",
    "db_config = EmbeddingPipelineConfigManager().database_config\n",
    "data_config = DataPipelineConfigManager()\n",
    "shop_config = data_config.shop_config\n",
    "mongodb_manager = MongoDBManager(data_config.mongodb_config)\n",
    "embedding_db = EmbeddingDatabase(db_config, shop_config)\n",
    "embedding_db.load()\n",
    "\n",
    "# Get data\n",
    "product_collection = mongodb_manager.get_collection(\n",
    "    mongodb_manager.config.product_collection\n",
    ")\n",
    "\n",
    "image_collection = mongodb_manager.get_collection(\n",
    "    mongodb_manager.config.image_metadata_collection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640f4bd",
   "metadata": {},
   "source": [
    "## Basic Text Embedding Generation\n",
    "\n",
    "Let's create a function to generate text embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text: str) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a text input.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        text_tokens = open_clip.tokenize([text]).to(device)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        # Normalize embeddings\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_product(product: dict) -> None:\n",
    "    # Generate text embedding for the description\n",
    "    text_embedding = get_text_embedding(product['description'])\n",
    "\n",
    "    # For random image, analyze image and its localizations\n",
    "    image_hash = np.random.choice(product['images'])    \n",
    "    image_data = image_collection.find_one({\"image_hash\": image_hash})\n",
    "    \n",
    "    # For each localization\n",
    "    results = {}\n",
    "    for loc in image_data['localizations']:\n",
    "        # Get the localization hash from metadata\n",
    "        loc_hash = loc['hash']\n",
    "            \n",
    "        # Search for this localization in embedding database\n",
    "        loc_embedding = embedding_db.get_embedding(loc_hash)\n",
    "        distance = np.linalg.norm(loc_embedding - text_embedding)\n",
    "        results[loc_hash] = distance\n",
    "    \n",
    "    # Sort results by distance\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1])\n",
    "    print(f\"Top localizations for image {image_hash}:\")\n",
    "    for loc_hash, distance in sorted_results:\n",
    "        print(f\"\\tLocalization: {loc_hash}, Distance: {distance:.4f}\")\n",
    "    \n",
    "    data_utils.display_image_summary(mongodb_manager, image_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random products with descriptions\n",
    "products_with_desc = list(product_collection.find({\n",
    "    \"description\": {\"$ne\": \"NOT_FOUND\"}\n",
    "}))\n",
    "\n",
    "# Test 5 random products\n",
    "import random\n",
    "random_products = random.sample(products_with_desc, 5)\n",
    "\n",
    "for product in random_products:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"\\nTesting product: {product['title']}: {product['hash']}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    test_product(product)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
